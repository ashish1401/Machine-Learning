{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr9wRD137xsP0N93i/xNUw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish1401/Machine-Learning/blob/main/Regression_Models_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression\n",
        "\n",
        "First attempt to implement the linear regression model\n",
        "\n",
        "y_pred = ∑xi*βi ; where x0 = 1;\n",
        "\n",
        "y_pred = β0 + x1β1 + x2β2 + .....\n",
        "\n",
        "where xi is the feature set\n",
        "\n",
        "\n",
        "How to define model parameters βi ?\n",
        "- Minimisizing the L2 Norm Error\n",
        "- Minimizing the cost function ie squared difference of y_pred and actual y\n",
        "- This is done via an optimaztion solution called gradient descent\n",
        "- We constantly update the values of our model parameters via the equations\n",
        "\n",
        "\n",
        "  βi := βi + L*d(cost-function)/dβi\n",
        "\n",
        "  Bi := Bi + L*(Xi)*(y - y_pred)\n",
        "\n",
        "  where L is the learning rate\n",
        "\n",
        "\n",
        "  SCOPE OF IMPROVEMENTS\n",
        "\n",
        "\n",
        "- Define the bias term independently instead of as a model coefficient with corressponding feature x0 = 1\n",
        "- Use the self.predict function instead of rewriting the prediction code in self.fit"
      ],
      "metadata": {
        "id": "hu52xbtIEXLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math"
      ],
      "metadata": {
        "id": "q4XqgO04JpX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-sdSTXwDEUs"
      },
      "outputs": [],
      "source": [
        "class LinearRegression():\n",
        "\n",
        "  def __init__(self,learning,samples):\n",
        "    self.learning = learning\n",
        "    self.samples = samples\n",
        "    self.n = None\n",
        "    self.m = None\n",
        "    self.weights = None\n",
        "    self.features = None\n",
        "\n",
        "  def update_weights(self,diff,df):\n",
        "    for j in range(df.shape[0]):\n",
        "      for i in range(self.n+1):\n",
        "        if(i==0):\n",
        "          self.weights[i] -= self.learning*diff[j]/self.m\n",
        "        else:\n",
        "          self.weights[i] -= self.learning*diff[j]*df[self.features[i-1]][j]/self.m\n",
        "\n",
        "\n",
        "\n",
        "  def fit(self,X,Y,feature,numFeatures=1):\n",
        "    self.n = numFeatures\n",
        "    self.m = X.shape[0]\n",
        "    # self.weights = np.random.rand(self.n+1)\n",
        "    self.weights = np.zeros(self.n+1)\n",
        "    self.features = feature\n",
        "    y_pred = np.zeros(self.m)\n",
        "    y = Y\n",
        "    minCost = 1e5\n",
        "    diff = np.zeros(self.m)\n",
        "\n",
        "\n",
        "\n",
        "    for step in range(self.samples):\n",
        "      cost_func = 0\n",
        "      y_pred.fill(0)\n",
        "      diff.fill(0)\n",
        "\n",
        "      for j in range(0,self.m):\n",
        "        for i in range(0,self.n+1):\n",
        "          if(i==0):\n",
        "            y_pred[j] += self.weights[i]\n",
        "          else:\n",
        "            y_pred[j] += X[self.features[i-1]][j]*self.weights[i]\n",
        "\n",
        "        cost_func = cost_func + (y_pred[j] - y[j])**2\n",
        "        diff[j] += y_pred[j] - y[j]\n",
        "      cost_func = cost_func/X.shape[0]\n",
        "      if(minCost>cost_func):\n",
        "         self.update_weights(diff=diff,df=X)\n",
        "      else:\n",
        "        break\n",
        "      minCost = min(cost_func,minCost)\n",
        "\n",
        "\n",
        "\n",
        "    return minCost\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "\n",
        "  def predict(self,X_test):\n",
        "    y_pred = np.zeros(X_test.shape[0]);\n",
        "    for j in range(0,X_test.shape[0]):\n",
        "      for i in range(0,self.n+1):\n",
        "          if(i==0):\n",
        "            y_pred[j] += self.weights[i]\n",
        "          else:\n",
        "            y_pred[j] += X_test[self.features[i-1]][j]*self.weights[i]\n",
        "    # for logistic regression return the sigmoid wrapped linear reg to recieve probability with a threshold of 0.5\n",
        "    # prob = self.sigmoid(y_pred)\n",
        "    # return prob\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "\n",
        "}\n",
        "X = pd.DataFrame(data)\n",
        "Y = np.array([0, 0, 0, 1, 1])  # Sample target variable\n",
        "features = ['feature1']\n",
        "\n",
        "# Create the LinearRegression object\n",
        "model = LinearRegression(learning=0.05, samples=2100)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, features, numFeatures=1)\n",
        "\n",
        "# Predict\n",
        "X_test = pd.DataFrame({\n",
        "    'feature1': [1.5, 3]\n",
        "})\n",
        "predictions = model.predict(X_test)\n",
        "print(f'Predictions: {predictions}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv2WzyvikM4j",
        "outputId": "8cb054c3-4985-4c59-8556-ea49ff933d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [-0.04999994  0.40000002]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression - Classification\n",
        "\n",
        "Use if a different cost function is made in order to correctly understand the deviation of predicted value from the true value which is not a unbounded ordinal value , thus a loss function different from the one used in Linear Regression must be used to find the best-fit line\n",
        "\n",
        "On using a same loss function as that of Linear Regression the gradient descent produces multiple local minima instead of a global minima\n",
        "\n",
        "Use of Binary Cross Entropy Loss Function or Log Loss"
      ],
      "metadata": {
        "id": "AapxNEAsWOGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L(y,𝓎) = -(ylog𝓎 + (1-y)log(1-𝓎))\n",
        "\n",
        "y = actual odds\n",
        "\n",
        "\n",
        "𝓎 = predicted odds\n",
        "\n",
        "When y = 1 :\n",
        "L (1,ŷ)\n",
        "\n",
        "= - (1logŷ + (1-1)log(1-ŷ))\n",
        "\n",
        "= - log ŷ\n",
        "\n",
        "\n",
        "We always want a smaller Loss Function value, hence, ŷ should be very large,\n",
        "so that (-log ŷ) will be a large negative number.\n",
        "\n",
        "\n",
        "When y = 0,\n",
        "\n",
        "L (0,ŷ) = -(0logŷ + (1-0)log(1-ŷ)) =>\n",
        "\n",
        "L(0,ŷ) = -log (1-ŷ)\n",
        "\n",
        "\n",
        "We always want a smaller Loss Function value, hence, ŷ should be very small, so that - log (1 - ŷ) will be a large negative number.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RlVOCj2HZVDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression():\n",
        "\n",
        "  def __init__(self,learning,num_iterations):\n",
        "    self.learning = learning;\n",
        "    self.num_iterations = num_iterations\n",
        "\n",
        "  def update_weight(self):\n",
        "    #code for Gradient Descent\n",
        "      for i in range(self.n+1):\n",
        "        if(i==0):\n",
        "          self.weights[i] -= self.learning*np.sum(self.y_pred_log - self.Y)/self.m\n",
        "        else:\n",
        "          self.weights[i] -= self.learning*np.dot(self.X[features[i-1]],(self.y_pred_log - self.Y))/self.m\n",
        "\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "  def fit(self,X,Y,features):\n",
        "    #code for Modelling\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.features = features\n",
        "    self.n = len(features)\n",
        "    self.m = X.shape[0]\n",
        "    self.weights = np.random.rand(self.n+1)\n",
        "    cost_func = 0\n",
        "    minCost = 1e5\n",
        "    y_pred = np.zeros(self.m)\n",
        "    self.y_pred_log = np.zeros(self.m)\n",
        "    for step in range (self.num_iterations):\n",
        "      y_pred.fill(0)\n",
        "      for j in range(0,self.m):\n",
        "         for i in range(0,self.n+1):\n",
        "           if(i==0):\n",
        "             y_pred[j] += self.weights[i]\n",
        "           else:\n",
        "             y_pred[j] += X[self.features[i-1]][j]*self.weights[i]\n",
        "      self.y_pred_log = self.sigmoid(y_pred)\n",
        "      cost_func += -np.mean(Y*np.log(self.y_pred_log) + (1-Y)*np.log(1-self.y_pred_log))\n",
        "      if(cost_func < minCost):\n",
        "        minCost = min(minCost,cost_func)\n",
        "        self.update_weight()\n",
        "      else:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "  def predict(self,X):\n",
        "    y_pred = np.zeros(X.shape[0])\n",
        "    for j in range(X.shape[0]):\n",
        "      for i in range(self.n+1):\n",
        "        if(i==0):\n",
        "          y_pred[j] += self.weights[i]\n",
        "        else:\n",
        "          y_pred[j] += X[self.features[i-1]][j]*self.weights[i]\n",
        "    y_pred_log = self.sigmoid(y_pred)\n",
        "    return y_pred_log\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S9yG0CnOzR3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'Feature1': [1, 2, 3, 4],\n",
        "            'Feature2': [2, 3, 4, 5]}\n",
        "X = pd.DataFrame(data)\n",
        "Y = np.array([0, 0, 1, 1])\n",
        "features = ['Feature1', 'Feature2']\n",
        "# Create and fit the model\n",
        "lr = LogisticRegression(learning=0.01, num_iterations=1000)\n",
        "lr.fit(X, Y, features)\n",
        "\n",
        "# Make predictions\n",
        "X_test = pd.DataFrame({'Feature1': [5, 6],\n",
        "                           'Feature2': [6, 7]})\n",
        "predictions = lr.predict(X_test)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUwmyIrHmJb3",
        "outputId": "b3ad42c2-8b7a-4fb1-bd3e-855eec8be884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.99951964 0.99984951]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CEN2mh5GmKc_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}